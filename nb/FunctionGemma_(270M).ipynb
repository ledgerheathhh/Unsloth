{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ledgerheathhh/Unsloth/blob/main/nb/FunctionGemma_(270M).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5_KaApGDw4a"
      },
      "source": [
        "To run this, press \"*Runtime*\" and press \"*Run all*\" on a **free** Tesla T4 Google Colab instance!\n",
        "<div class=\"align-center\">\n",
        "<a href=\"https://unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "<a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord button.png\" width=\"145\"></a>\n",
        "<a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a></a> Join Discord if you need help + ⭐ <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ⭐\n",
        "</div>\n",
        "\n",
        "To install Unsloth your local device, follow [our guide](https://docs.unsloth.ai/get-started/install-and-update). This notebook is licensed [LGPL-3.0](https://github.com/unslothai/notebooks?tab=LGPL-3.0-1-ov-file#readme).\n",
        "\n",
        "You will learn how to do [data prep](#Data), how to [train](#Train), how to [run the model](#Inference), & [how to save it](#Save)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPV1sqablUcK"
      },
      "source": [
        "### News"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgPcPRsClUcL"
      },
      "source": [
        "\n",
        "Unsloth's [Docker image](https://hub.docker.com/r/unsloth/unsloth) is here! Start training with no setup & environment issues. [Read our Guide](https://docs.unsloth.ai/new/how-to-train-llms-with-unsloth-and-docker).\n",
        "\n",
        "[gpt-oss RL](https://docs.unsloth.ai/new/gpt-oss-reinforcement-learning) is now supported with the fastest inference & lowest VRAM. Try our [new notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/gpt-oss-(20B)-GRPO.ipynb) which creates kernels!\n",
        "\n",
        "Introducing [Vision](https://docs.unsloth.ai/new/vision-reinforcement-learning-vlm-rl) and [Standby](https://docs.unsloth.ai/basics/memory-efficient-rl) for RL! Train Qwen, Gemma etc. VLMs with GSPO - even faster with less VRAM.\n",
        "\n",
        "Unsloth now supports Text-to-Speech (TTS) models. Read our [guide here](https://docs.unsloth.ai/basics/text-to-speech-tts-fine-tuning).\n",
        "\n",
        "Visit our docs for all our [model uploads](https://docs.unsloth.ai/get-started/all-our-models) and [notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEdS3fN5lUcL"
      },
      "source": [
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HBeBCkyhmONg"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os, re\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    import torch; v = re.match(r\"[0-9]{1,}\\.[0-9]{1,}\", str(torch.__version__)).group(0)\n",
        "    xformers = \"xformers==\" + (\"0.0.33.post1\" if v==\"2.9\" else \"0.0.32.post2\" if v==\"2.8\" else \"0.0.29.post3\")\n",
        "    !pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets==4.3.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
        "    !pip install --no-deps unsloth\n",
        "!pip install transformers==4.57.3\n",
        "!pip install --no-deps trl==0.22.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGMWlrRdzwgf"
      },
      "source": [
        "### Unsloth\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Xbb0cuLzwgf"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 4096 # Can choose any sequence length!\n",
        "fourbit_models = [\n",
        "    # 4bit Gemma 3 dynamic quants for superior accuracy and low memory use\n",
        "    \"unsloth/gemma-3-270m-it-unsloth-bnb-4bit\",\n",
        "    \"unsloth/gemma-3-1b-it-unsloth-bnb-4bit\",\n",
        "    \"unsloth/gemma-3-4b-it-unsloth-bnb-4bit\",\n",
        "    \"unsloth/gemma-3-12b-it-unsloth-bnb-4bit\",\n",
        "    \"unsloth/gemma-3-27b-it-unsloth-bnb-4bit\",\n",
        "    # Function Gemma models\n",
        "    \"unsloth/functiongemma-270m-it\",\n",
        "    \"unsloth/functiongemma-270m-it-unsloth-bnb-4bit\",\n",
        "    \"unsloth/functiongemma-270m-it-bnb-4bit\",\n",
        "] # More models at https://huggingface.co/unsloth\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/functiongemma-270m-it\",\n",
        "    max_seq_length = max_seq_length, # Choose any for long context!\n",
        "    load_in_4bit = False,  # 4 bit quantization to reduce memory\n",
        "    load_in_8bit = False, # [NEW!] A bit more accurate, uses 2x memory\n",
        "    load_in_16bit = True, # [NEW!] Enables 16bit LoRA\n",
        "    full_finetuning = False, # [NEW!] We have full finetuning now!\n",
        "    # token = \"hf_...\", # use one if using gated models\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXd9bTZd1aaL"
      },
      "source": [
        "We now add LoRA adapters so we only need to update a small amount of parameters!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6bZsfBuZDeCL"
      },
      "outputs": [],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 128, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 128*2,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWxBiHgSlUcM"
      },
      "source": [
        "Formatting is very important in the `functiongemma` for tool-calling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvJ5lFSBlUcM"
      },
      "source": [
        "For the general conversation,  each role (`developer`, `user`, `model`) is wrapped with `<start_of_turn>{role} ... <end_of_turn>`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f8AqDWEolUcM"
      },
      "outputs": [],
      "source": [
        "messages_1 = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Hello, who are you?\"},\n",
        "]\n",
        "\n",
        "rendered_1 = tokenizer.apply_chat_template(\n",
        "    messages_1,\n",
        "    tools = [], # no tools\n",
        "    add_generation_prompt = False,\n",
        "    tokenize = False,\n",
        ")\n",
        "\n",
        "print(\"=== Example 1: Basic turns ===\")\n",
        "print(rendered_1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOyeDekElUcM"
      },
      "source": [
        "For tool calling, in the `developer` turn, `<start_function_declaration>declaration:get_weather{...}<end_function_declaration>` encodes the full function spec (name, description, parameters) so the model knows *what* tools it can call and how to format arguments.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z2a7XSaKlUcM"
      },
      "outputs": [],
      "source": [
        "tools_2 = [\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"get_weather\",\n",
        "            \"description\": \"Get the current weather for a given city.\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"city\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"description\": \"City name, e.g. 'Tokyo'.\",\n",
        "                    }\n",
        "                },\n",
        "                \"required\": [\"city\"],\n",
        "            },\n",
        "        },\n",
        "    }\n",
        "]\n",
        "\n",
        "messages_2 = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a weather assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": \"What is the weather in Tokyo?\"},\n",
        "]\n",
        "\n",
        "rendered_2 = tokenizer.apply_chat_template(\n",
        "    messages_2,\n",
        "    tools = tools_2,\n",
        "    add_generation_prompt = False,\n",
        "    tokenize = False,\n",
        ")\n",
        "\n",
        "print(\"=== Example 2: Tool declarations ===\")\n",
        "print(rendered_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOiVlSaGlUcM"
      },
      "source": [
        "For tool calling + result of the tool call + LLM answer, the user turn is plain text; the next `model` turn embeds `<start_function_call>call:get_weather{...}<end_function_call>` and a matching `<start_function_response>response:get_weather{...}<end_function_response>`, modeling the whole “call tool → receive result → answer user” loop inside the prompt.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OILFbQ4llUcM"
      },
      "outputs": [],
      "source": [
        "messages_3 = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"You are a weather assistant.\",\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"What is the weather in Tokyo?\",\n",
        "    },\n",
        "    # Assistant issues a tool call\n",
        "    {\n",
        "        \"role\": \"assistant\",\n",
        "        \"content\": \"\",\n",
        "        \"tool_calls\": [\n",
        "            {\n",
        "                \"id\": \"call_1\",\n",
        "                \"type\": \"function\",\n",
        "                \"function\": {\n",
        "                    \"name\": \"get_weather\",\n",
        "                    \"arguments\": {\"city\": \"Tokyo\"},\n",
        "                },\n",
        "            }\n",
        "        ],\n",
        "    },\n",
        "    # Tool (infrastructure) responds\n",
        "    {\n",
        "        \"role\": \"tool\",\n",
        "        \"name\": \"get_weather\",\n",
        "        \"tool_call_id\": \"call_1\",\n",
        "        \"content\": '{\"city\": \"Tokyo\", \"temp_c\": 25, \"condition\": \"sunny\"}',\n",
        "    },\n",
        "    # Assistant gives final natural-language answer\n",
        "    {\n",
        "        \"role\": \"assistant\",\n",
        "        \"content\": \"It is currently 25°C and sunny in Tokyo.\",\n",
        "    },\n",
        "]\n",
        "\n",
        "rendered_3 = tokenizer.apply_chat_template(\n",
        "    messages_3,\n",
        "    tools = tools_2,\n",
        "    add_generation_prompt = False,\n",
        "    tokenize = False,\n",
        ")\n",
        "\n",
        "print(\"=== Example 3: User → Model → Tool → Model ===\")\n",
        "print(rendered_3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-eWne3IQlUcM"
      },
      "source": [
        "Lastly, for our thinking process which we will do in this notebook, since they did not support it in the chat template, we will put it inside the assistant response instead. We will use the tag `<think>...</think>`.  The `<think>...</think>` blocks inside the `model` turn are just tagged text that represent internal reasoning; they appear before and after the tool interaction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fTkZTFyYlUcN"
      },
      "outputs": [],
      "source": [
        "tools_4 = [\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"get_amazon_product_details\",\n",
        "            \"description\": (\n",
        "                \"Retrieves comprehensive product information from Amazon, \"\n",
        "                \"including title, price, description, specifications, and availability.\"\n",
        "            ),\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"asin\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"description\": \"The Amazon ASIN of the product.\",\n",
        "                    }\n",
        "                },\n",
        "                \"required\": [\"asin\"],\n",
        "            },\n",
        "        },\n",
        "    }\n",
        "]\n",
        "\n",
        "messages_4 = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": (\n",
        "            \"You are a shopping assistant. Use tools when you need detailed \"\n",
        "            \"Amazon product data such as price and specifications.\"\n",
        "        ),\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"Is the espresso machine with ASIN B0XYZ12345 any good for home use?\",\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"assistant\",\n",
        "        \"content\": (\n",
        "            \"<think>\"\n",
        "            \"User is asking for an opinion, but I need factual product details first \"\n",
        "            \"such as price, features, and reviews. I should call the Amazon product \"\n",
        "            \"details tool with the provided ASIN.\"\n",
        "            \"</think>\"\n",
        "        ),\n",
        "        \"tool_calls\": [\n",
        "            {\n",
        "                \"id\": \"call_1\",\n",
        "                \"type\": \"function\",\n",
        "                \"function\": {\n",
        "                    \"name\": \"get_amazon_product_details\",\n",
        "                    \"arguments\": {\n",
        "                        \"asin\": \"B0XYZ12345\"\n",
        "                    },\n",
        "                },\n",
        "            }\n",
        "        ],\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"tool\",\n",
        "        \"name\": \"get_amazon_product_details\",\n",
        "        \"tool_call_id\": \"call_1\",\n",
        "        \"content\": (\n",
        "            '{\"title\": \"Home Pro Espresso 3000\", '\n",
        "            '\"price\": 199.99, '\n",
        "            '\"pressure_bar\": 15, '\n",
        "            '\"features\": [\"steam wand\", \"single and double shot baskets\"], '\n",
        "            '\"pros\": [\"good crema\", \"compact\"], '\n",
        "            '\"cons\": [\"a bit noisy\"]}'\n",
        "        ),\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"assistant\",\n",
        "        \"content\": (\n",
        "            \"<think>\"\n",
        "            \"Tool response shows a mid-range price and standard 15 bar pressure. \"\n",
        "            \"Features and pros/cons indicate it’s fine for home espresso but not \"\n",
        "            \"a high-end machine for enthusiasts.\"\n",
        "            \"</think>\\n\"\n",
        "            \"Based on the product details, the Home Pro Espresso 3000 (ASIN B0XYZ12345) \"\n",
        "            \"is a solid option for home use. It offers 15-bar pressure, a steam wand, \"\n",
        "            \"and both single and double shot baskets, which are enough for everyday \"\n",
        "            \"lattes and cappuccinos. It’s compact and produces good crema, but it can \"\n",
        "            \"be a bit noisy. If you want a convenient, reasonably priced home machine, \"\n",
        "            \"it should work well; if you’re very picky about espresso or plan to upgrade \"\n",
        "            \"grinders and accessories, you might eventually want something more advanced.\"\n",
        "        ),\n",
        "    },\n",
        "]\n",
        "\n",
        "rendered_prompt = tokenizer.apply_chat_template(\n",
        "    messages_4,\n",
        "    tools = tools_4,\n",
        "    add_generation_prompt = False,  # True if you want to open a fresh model turn for generation\n",
        "    tokenize = False,\n",
        ")\n",
        "\n",
        "print(\"=== Thinking + Tools ===\")\n",
        "print(rendered_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYKrWYrGlUcN"
      },
      "source": [
        "<a name=\"Data\"></a>\n",
        "### Data Prep\n",
        "We now use the built-in `functiongemma` format for conversation style finetunes. We use [TxT360-3efforts](https://huggingface.co/datasets/LLM360/TxT360-3efforts) dataset. This dataset contains SFT dataset for tool-calling with thinking reasoning, which the `functiongemma` original model did not have this thinking capability.\n",
        "\n",
        "Since this dataset contains more than 1 million examples, we will take only the first 50000 using `streaming=True`, bypassing downloading the entire dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8IZC9-mslUcN"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, Dataset\n",
        "dataset = load_dataset(\"LLM360/TxT360-3efforts\", name = \"agent\", split = \"medium\", streaming = True)\n",
        "dataset = Dataset.from_list(list(dataset.take(50000)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jilhkFMOlUcN"
      },
      "source": [
        "Let's check one example of the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ccAVCSulUcN"
      },
      "outputs": [],
      "source": [
        "dataset[0][\"messages\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTXAVRH8lUcN"
      },
      "source": [
        "If we look closely, our dataset is in the form of string. Secondly, the format of this dataset is different with the one that is required for `functiongemma`. Especially the one when we need to pass the tools. We will use our defined function to do this :\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oXo63h-0lUcN"
      },
      "outputs": [],
      "source": [
        "#@title Helper Function: prepare_messages_and_tools\n",
        "\n",
        "import json\n",
        "\n",
        "THINK_TAG_OPEN = \"<think>\"\n",
        "THINK_TAG_CLOSE = \"</think>\"\n",
        "\n",
        "def prepare_messages_and_tools(example):\n",
        "    raw = json.loads(example[\"messages\"])\n",
        "    msgs = [dict(m) for m in raw]\n",
        "\n",
        "    # 1) Extract tools (same as before)\n",
        "    tools_raw = []\n",
        "    if msgs and isinstance(msgs[0], dict):\n",
        "        tlist = msgs[0].get(\"tools\")\n",
        "        if isinstance(tlist, list) and tlist:\n",
        "            tools_raw = tlist\n",
        "            msgs[0].pop(\"tools\", None)\n",
        "\n",
        "    # 2) Merge assistant[\"think\"] into [\"content\"]\n",
        "    THINK_KEYS = [\"think\", \"think_fast\", \"think_faster\"]\n",
        "\n",
        "    # TRACKER: Check if we successfully added thoughts\n",
        "    has_valid_thought = False\n",
        "\n",
        "    for m in msgs:\n",
        "        if m.get(\"role\") == \"assistant\":\n",
        "            # Find the first available thinking key\n",
        "            found_key = next((k for k in THINK_KEYS if m.get(k)), None)\n",
        "\n",
        "            if found_key:\n",
        "                think_text = m[found_key]\n",
        "                content = m.get(\"content\")\n",
        "                think_block = f\"{THINK_TAG_OPEN}{think_text}{THINK_TAG_CLOSE}\"\n",
        "\n",
        "                if isinstance(content, str) and content:\n",
        "                    m[\"content\"] = think_block + \"\\n\" + content\n",
        "                else:\n",
        "                    m[\"content\"] = think_block\n",
        "\n",
        "                has_valid_thought = True\n",
        "\n",
        "                # Clean up keys\n",
        "                for k in THINK_KEYS:\n",
        "                    m.pop(k, None)\n",
        "            else:\n",
        "                # If an assistant message HAS NO THOUGHT,\n",
        "                # this example is \"poison\" for your goal.\n",
        "                # We mark it as invalid to filter it out later.\n",
        "                return None, None\n",
        "\n",
        "    # If the conversation had no assistant turns at all (rare, but possible), skip it\n",
        "    if not has_valid_thought:\n",
        "        return None, None\n",
        "    # 3) Normalize tool_calls to HF-style {type:'function', function:{name, arguments}}\n",
        "    for m in msgs:\n",
        "        if \"tool_calls\" not in m or not m[\"tool_calls\"]:\n",
        "            continue\n",
        "\n",
        "        new_tool_calls = []\n",
        "        for tc in m[\"tool_calls\"]:\n",
        "            if not isinstance(tc, dict):\n",
        "                continue\n",
        "\n",
        "            # Already has function dict?\n",
        "            if \"function\" in tc and isinstance(tc[\"function\"], dict):\n",
        "                new_tool_calls.append(tc)\n",
        "                continue\n",
        "\n",
        "            fn_name = tc.get(\"name\", \"\")\n",
        "            args = tc.get(\"arguments\", {})\n",
        "\n",
        "            # Try to parse JSON string arguments\n",
        "            if isinstance(args, str):\n",
        "                try:\n",
        "                    args = json.loads(args)\n",
        "                except Exception:\n",
        "                    pass\n",
        "\n",
        "            new_tool_calls.append(\n",
        "                {\n",
        "                    \"id\": tc.get(\"id\") or tc.get(\"tool_call_id\"),\n",
        "                    \"type\": tc.get(\"type\", \"function\"),\n",
        "                    \"function\": {\n",
        "                        \"name\": fn_name,\n",
        "                        \"arguments\": args,\n",
        "                    },\n",
        "                }\n",
        "            )\n",
        "\n",
        "        m[\"tool_calls\"] = new_tool_calls\n",
        "\n",
        "    # 3b) Build map from tool_call_id -> function name for later tool responses\n",
        "    id_to_name = {}\n",
        "    for m in msgs:\n",
        "        for tc in m.get(\"tool_calls\", []) or []:\n",
        "            if not isinstance(tc, dict):\n",
        "                continue\n",
        "            fn = tc.get(\"function\") or {}\n",
        "            name = fn.get(\"name\") or tc.get(\"name\")\n",
        "            tc_id = tc.get(\"id\") or tc.get(\"tool_call_id\")\n",
        "            if tc_id and name:\n",
        "                id_to_name[tc_id] = name\n",
        "\n",
        "    # 3c) Ensure tool response messages have a 'name'\n",
        "    for m in msgs:\n",
        "        if m.get(\"role\") == \"tool\":\n",
        "            if not m.get(\"name\"):\n",
        "                # Try to infer from tool_call_id using previous map\n",
        "                tc_id = m.get(\"tool_call_id\")\n",
        "                inferred = id_to_name.get(tc_id) if tc_id else None\n",
        "                m[\"name\"] = inferred or \"unknown_tool\"\n",
        "\n",
        "    # 4) Normalize tool schemas to HF-style {type:'function', function:{...}}\n",
        "    adapted_tools = []\n",
        "    for t in tools_raw:\n",
        "        if not isinstance(t, dict):\n",
        "            continue\n",
        "\n",
        "        if \"function\" in t and isinstance(t[\"function\"], dict):\n",
        "            adapted_tools.append(t)\n",
        "            continue\n",
        "\n",
        "        name = t.get(\"name\", \"\")\n",
        "        description = t.get(\"description\", \"\")\n",
        "        parameters = t.get(\"parameters\") or {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {},\n",
        "        }\n",
        "\n",
        "        adapted_tools.append(\n",
        "            {\n",
        "                \"type\": t.get(\"type\", \"function\"),\n",
        "                \"function\": {\n",
        "                    \"name\": name,\n",
        "                    \"description\": description,\n",
        "                    \"parameters\": parameters,\n",
        "                },\n",
        "            }\n",
        "        )\n",
        "\n",
        "    # Delete empty system message\n",
        "    first_message = msgs[0]\n",
        "    if first_message[\"role\"] == \"system\" and \"content\" not in first_message:\n",
        "        msgs.pop(0)\n",
        "\n",
        "    return msgs, adapted_tools"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adUioMHKlUcN"
      },
      "source": [
        "This will separate the conversation and the tools which we will pass to `tokenizer.apply_chat_template`.\n",
        "\n",
        "Let's transform all of our dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m-60AX9ulUcN"
      },
      "outputs": [],
      "source": [
        "def format_example(example):\n",
        "    messages, tools = prepare_messages_and_tools(example)\n",
        "\n",
        "    # FILTER: If the preparation returned None, this example was bad.\n",
        "    if messages is None or len(messages) == 0:\n",
        "        return {\"text\": None}\n",
        "\n",
        "    chat_str = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tools = tools,\n",
        "        add_generation_prompt = False,\n",
        "        tokenize = False,\n",
        "    ).removeprefix(\"<bos>\")\n",
        "\n",
        "    return {\n",
        "        \"text\": chat_str,\n",
        "    }\n",
        "\n",
        "# Apply the map\n",
        "train_dataset = dataset.map(format_example)\n",
        "\n",
        "# Filter out the None values\n",
        "train_dataset = train_dataset.filter(lambda x: x[\"text\"] is not None)\n",
        "\n",
        "print(f\"Dataset size after filtering: {len(train_dataset)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7Y6l-bmlUcN"
      },
      "source": [
        "Let's see the resulting output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uQWvHqEdlUcN"
      },
      "outputs": [],
      "source": [
        "train_dataset[0][\"text\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idAEIeSQ3xdS"
      },
      "source": [
        "<a name=\"Train\"></a>\n",
        "### Train the model\n",
        "Now let's train our model. We do 500 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95_Nn-89DhsL"
      },
      "outputs": [],
      "source": [
        "from trl import SFTTrainer, SFTConfig\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = train_dataset,\n",
        "    eval_dataset = None, # Can set up evaluation!\n",
        "    args = SFTConfig(\n",
        "        dataset_text_field = \"text\",\n",
        "        per_device_train_batch_size = 4,\n",
        "        gradient_accumulation_steps = 2, # Use GA to mimic batch size!\n",
        "        warmup_steps = 10,\n",
        "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
        "        max_steps = 500,\n",
        "        learning_rate = 2e-4, # Reduce to 2e-5 for long training runs\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.001,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "        report_to = \"none\", # Use TrackIO/WandB etc\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l35FwgQ1phi6"
      },
      "source": [
        "We also use Unsloth's `train_on_completions` method to only train on the assistant outputs and ignore the loss on the user's inputs. This helps increase accuracy of finetunes!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YA33AvDwsmn-"
      },
      "outputs": [],
      "source": [
        "from unsloth.chat_templates import train_on_responses_only\n",
        "trainer = train_on_responses_only(\n",
        "    trainer,\n",
        "    instruction_part = \"<start_of_turn>user\\n\",\n",
        "    response_part = \"<start_of_turn>model\\n\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGtWXmRJplcA"
      },
      "source": [
        "Let's verify masking the instruction part is done! Let's print the 100th row again."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yDvlCjbS7l86"
      },
      "outputs": [],
      "source": [
        "tokenizer.decode(trainer.train_dataset[-1][\"input_ids\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9ltHtqSpoHg"
      },
      "source": [
        "Now let's print the masked out example - you should see only the answer is present:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BjwHcW-X7mj4"
      },
      "outputs": [],
      "source": [
        "[tokenizer.decode([tokenizer.pad_token_id if x == -100 else x for x in trainer.train_dataset[100][\"labels\"]]).replace(tokenizer.pad_token, \"-\")]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2ejIt2xSNKKp"
      },
      "outputs": [],
      "source": [
        "# @title Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNP1Uidk9mrz"
      },
      "source": [
        "Let's train the model! To resume a training run, set `trainer.train(resume_from_checkpoint = True)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqxqAZ7KJ4oL"
      },
      "outputs": [],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "pCqnaKmlO1U9"
      },
      "outputs": [],
      "source": [
        "# @title Show final memory and time stats\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory / max_memory * 100, 3)\n",
        "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(\n",
        "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
        ")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekOmTR1hSNcr"
      },
      "source": [
        "<a name=\"Inference\"></a>\n",
        "### Inference\n",
        "Let's run the model via Unsloth native inference!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNntWaokpO5N"
      },
      "source": [
        "We will take only the first two `messages`, which is the `system` role and the `user` role while also passing the `tools` to the prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QaFG40dtlUcO"
      },
      "outputs": [],
      "source": [
        "messages, tools = prepare_messages_and_tools(train_dataset[0])\n",
        "\n",
        "text = tokenizer.apply_chat_template(\n",
        "    messages[:1],\n",
        "    tools = tools,\n",
        "    tokenize = False,\n",
        "    add_generation_prompt = True, # Must add for generation\n",
        ").removeprefix('<bos>')\n",
        "\n",
        "from transformers import TextStreamer\n",
        "_ = model.generate(\n",
        "    **tokenizer(text, return_tensors = \"pt\").to(\"cuda\"),\n",
        "    max_new_tokens = 1024,\n",
        "    streamer = TextStreamer(tokenizer, skip_prompt = False),\n",
        "    top_p = 0.95, top_k = 64, temperature = 1.0,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMuVrWbjAzhc"
      },
      "source": [
        "<a name=\"Save\"></a>\n",
        "### Saving, loading finetuned models\n",
        "To save the final model as LoRA adapters, either use Huggingface's `push_to_hub` for an online save or `save_pretrained` for a local save.\n",
        "\n",
        "**[NOTE]** This ONLY saves the LoRA adapters, and not the full model. To save to 16bit or GGUF, scroll down!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upcOlWe7A1vc"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(\"functiongemma\")  # Local saving\n",
        "tokenizer.save_pretrained(\"functiongemma\")\n",
        "# model.push_to_hub(\"your_name/functiongemma\", token = \"...\") # Online saving\n",
        "# tokenizer.push_to_hub(\"your_name/functiongemma\", token = \"...\") # Online saving"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEEcJ4qfC7Lp"
      },
      "source": [
        "Now if you want to load the LoRA adapters we just saved for inference, set `False` to `True`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MKX_XKs_BNZR"
      },
      "outputs": [],
      "source": [
        "if False:\n",
        "    from unsloth import FastLanguageModel\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name = \"functiongemma\", # YOUR MODEL YOU USED FOR TRAINING\n",
        "        max_seq_length = 2048,\n",
        "        load_in_4bit = False,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f422JgM9sdVT"
      },
      "source": [
        "### Saving to float16 for vLLM\n",
        "\n",
        "We also support saving to `float16` directly. Select `merged_16bit` for float16 or `merged_4bit` for int4. We also allow `lora` adapters as a fallback. Use `push_to_hub_merged` to upload to your Hugging Face account! You can go to https://huggingface.co/settings/tokens for your personal tokens. See [our docs](https://docs.unsloth.ai/basics/inference-and-deployment) for more deployment options."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iHjt_SMYsd3P"
      },
      "outputs": [],
      "source": [
        "# Merge to 16bit\n",
        "if False:\n",
        "    model.save_pretrained_merged(\"functiongemma-finetune\", tokenizer, save_method = \"merged_16bit\")\n",
        "if False: # Pushing to HF Hub\n",
        "    model.push_to_hub_merged(\"hf/functiongemma-finetune\", tokenizer, save_method = \"merged_16bit\", token = \"\")\n",
        "\n",
        "# Merge to 4bit\n",
        "if False:\n",
        "    model.save_pretrained_merged(\"functiongemma-finetune\", tokenizer, save_method = \"merged_4bit\",)\n",
        "if False: # Pushing to HF Hub\n",
        "    model.push_to_hub_merged(\"hf/functiongemma-finetune\", tokenizer, save_method = \"merged_4bit\", token = \"\")\n",
        "\n",
        "# Just LoRA adapters\n",
        "if False:\n",
        "    model.save_pretrained(\"functiongemma-finetune\")\n",
        "    tokenizer.save_pretrained(\"functiongemma-finetune\")\n",
        "if False: # Pushing to HF Hub\n",
        "    model.push_to_hub(\"hf/functiongemma-finetune\", token = \"\")\n",
        "    tokenizer.push_to_hub(\"hf/functiongemma-finetune\", token = \"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCv4vXHd61i7"
      },
      "source": [
        "### GGUF / llama.cpp Conversion\n",
        "To save to `GGUF` / `llama.cpp`, we support it natively now for all models! For now, you can convert easily to `Q8_0, F16 or BF16` precision. `Q4_K_M` for 4bit will come later!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nAlzN8HKu5Ll"
      },
      "outputs": [],
      "source": [
        "if False: # Change to True to save to GGUF\n",
        "    model.save_pretrained_gguf(\n",
        "        \"functiongemma-finetune\",\n",
        "        tokenizer,\n",
        "        quantization_method = \"Q8_0\", # For now only Q8_0, BF16, F16 supported\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q974YEVPI7JS"
      },
      "source": [
        "Likewise, if you want to instead push to GGUF to your Hugging Face account, set `if False` to `if True` and add your Hugging Face token and upload location!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S-TqCnZVu5Ll"
      },
      "outputs": [],
      "source": [
        "if False: # Change to True to upload GGUF\n",
        "    model.push_to_hub_gguf(\n",
        "        \"HF_ACCOUNT/functiongemma-gguf\",\n",
        "        tokenizer,\n",
        "        quantization_method = \"Q8_0\", # Only Q8_0, BF16, F16 supported\n",
        "        token = \"hf_...\",\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdaYCPDUDw4o"
      },
      "source": [
        "Now, use the `functiongemma-finetune.gguf` file or `functiongemma-finetune-Q4_K_M.gguf` file in llama.cpp.\n",
        "\n",
        "And we're done! If you have any questions on Unsloth, we have a [Discord](https://discord.gg/unsloth) channel! If you find any bugs or want to keep updated with the latest LLM stuff, or need help, join projects etc, feel free to join our Discord!\n",
        "\n",
        "Some other links:\n",
        "1. Train your own reasoning model - Llama GRPO notebook [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-GRPO.ipynb)\n",
        "2. Saving finetunes to Ollama. [Free notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb)\n",
        "3. Llama 3.2 Vision finetuning - Radiography use case. [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb)\n",
        "6. See notebooks for DPO, ORPO, Continued pretraining, conversational finetuning and more on our [documentation](https://docs.unsloth.ai/get-started/unsloth-notebooks)!\n",
        "\n",
        "<div class=\"align-center\">\n",
        "  <a href=\"https://unsloth.ai\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "  <a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord.png\" width=\"145\"></a>\n",
        "  <a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a>\n",
        "\n",
        "  Join Discord if you need help + ⭐️ <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ⭐️\n",
        "</div>\n",
        "\n",
        "  This notebook and all Unsloth notebooks are licensed [LGPL-3.0](https://github.com/unslothai/notebooks?tab=LGPL-3.0-1-ov-file#readme).\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}